{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase 3**"
      ],
      "metadata": {
        "id": "VJiKVDBB7iVP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Distributed Data Cleaning and Pre-Processing"
      ],
      "metadata": {
        "id": "euCbYG8B7ulW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "8ndTdw4OK8Xl"
      },
      "outputs": [],
      "source": [
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PNQGqmR2gZj",
        "outputId": "f6eab19d-cc05-4174-c4d0-54e48014f8a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------+---------+-----------------+---------+\n",
            "|CustomerID|Recency|Frequency|         Monetary|RFM_Score|\n",
            "+----------+-------+---------+-----------------+---------+\n",
            "|     15727|     16|        7|          5159.06|      144|\n",
            "|     13623|     30|        5|727.7400000000001|      243|\n",
            "|     13623|     30|        5|727.7400000000001|      243|\n",
            "|     15727|     16|        7|          5159.06|      144|\n",
            "|     15727|     16|        7|          5159.06|      144|\n",
            "+----------+-------+---------+-----------------+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize Spark Session\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"Retail Data Cleaning\").getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Read the CSV file\n",
        "xls_file = pd.read_excel('Online Retail.xlsx')\n",
        "xls_file.to_csv('Online_Retail.csv', index=False)\n",
        "df = spark.read.format(\"csv\") \\\n",
        "    .option(\"header\", \"true\") \\\n",
        "    .option(\"inferSchema\", \"true\") \\\n",
        "    .load('Online_Retail.csv')\n",
        "\n",
        "# DISTRIBUTED OPERATION 1: Remove duplicates using distributed distinct operation\n",
        "df = df.distinct()\n",
        "\n",
        "# DISTRIBUTED OPERATION 2: Clean and transform data using distributed map operations\n",
        "df = df.dropna() \\\n",
        "    .withColumn(\"CustomerID\", col(\"CustomerID\").cast(\"integer\")) \\\n",
        "    .withColumn(\"InvoiceDate\", to_timestamp(\"InvoiceDate\")) \\\n",
        "    .withColumn(\"Total_Price\", col(\"Quantity\") * col(\"UnitPrice\")) \\\n",
        "    .withColumn(\"Year\", year(\"InvoiceDate\")) \\\n",
        "    .withColumn(\"Month\", month(\"InvoiceDate\")) \\\n",
        "    .withColumn(\"DayOfWeek\", dayofweek(\"InvoiceDate\"))\n",
        "\n",
        "# DISTRIBUTED OPERATION 3: Season calculation using distributed UDF\n",
        "@udf(returnType=StringType())\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]: return 'Winter'\n",
        "    elif month in [3, 4, 5]: return 'Spring'\n",
        "    elif month in [6, 7, 8]: return 'Summer'\n",
        "    else: return 'Autumn'\n",
        "\n",
        "df = df.withColumn(\"Season\", get_season(col(\"Month\")))\n",
        "\n",
        "# DISTRIBUTED OPERATION 4: Filter invalid data using distributed filtering\n",
        "df = df.filter((col(\"Quantity\") > 0) & (col(\"UnitPrice\") > 0))\n",
        "\n",
        "# DISTRIBUTED OPERATION 5: Text cleaning using distributed UDF\n",
        "@udf(returnType=StringType())\n",
        "def clean_text(text):\n",
        "    if text is None: return None\n",
        "    # Remove special characters and extra spaces\n",
        "    cleaned = re.sub(r'[^\\w\\s]', '', str(text)).strip()\n",
        "    return cleaned\n",
        "\n",
        "string_columns = [field.name for field in df.schema.fields if isinstance(field.dataType, StringType)]\n",
        "for column in string_columns:\n",
        "    df = df.withColumn(column, clean_text(col(column)))\n",
        "\n",
        "# DISTRIBUTED OPERATION 6: RFM Analysis using window functions\n",
        "window_spec = Window.orderBy(\"InvoiceDate\")\n",
        "max_date = df.agg(max(\"InvoiceDate\")).collect()[0][0]\n",
        "\n",
        "rfm_df = df.groupBy(\"CustomerID\").agg(\n",
        "    datediff(lit(max_date), max(\"InvoiceDate\")).alias(\"Recency\"),\n",
        "    countDistinct(\"InvoiceNo\").alias(\"Frequency\"),\n",
        "    sum(\"Total_Price\").alias(\"Monetary\")\n",
        ")\n",
        "\n",
        "# Calculate quartiles using window functions\n",
        "window_quartile = Window.orderBy(\"Recency\")\n",
        "rfm_df = rfm_df.withColumn(\"R\", ntile(4).over(window_quartile))\n",
        "window_quartile = Window.orderBy(\"Frequency\")\n",
        "rfm_df = rfm_df.withColumn(\"F\", ntile(4).over(window_quartile))\n",
        "window_quartile = Window.orderBy(\"Monetary\")\n",
        "rfm_df = rfm_df.withColumn(\"M\", ntile(4).over(window_quartile))\n",
        "\n",
        "# Create RFM Score\n",
        "rfm_df = rfm_df.withColumn(\"RFM_Score\",\n",
        "    concat(col(\"R\").cast(\"string\"),\n",
        "          col(\"F\").cast(\"string\"),\n",
        "          col(\"M\").cast(\"string\")))\n",
        "\n",
        "# Join RFM metrics back to main dataframe\n",
        "df = df.join(rfm_df, \"CustomerID\", \"left\")\n",
        "\n",
        "# Cache the DataFrame for better performance in subsequent operations\n",
        "df.cache()\n",
        "\n",
        "# Show the results\n",
        "df.select(\"CustomerID\", \"Recency\", \"Frequency\", \"Monetary\", \"RFM_Score\").show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dag Visualization"
      ],
      "metadata": {
        "id": "KAXd3hxS738D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "v5K_Zx4QJl6y"
      },
      "outputs": [],
      "source": [
        "!pip install -q pyngrok\n",
        "from pyngrok import ngrok\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCEqBdILJ1b1",
        "outputId": "74746900-0284-4a41-ba3f-19c94afb2e36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2od6RWKaNlm979WXnmxhHKvD6r7_4ENE12xfLibJZ654juXn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6X8LsB6VJ8bE",
        "outputId": "e0ce7942-5b8e-46a6-aeed-1e1bcb938dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark UI running on: http://3da39760f058:4040\n"
          ]
        }
      ],
      "source": [
        "print(\"Spark UI running on:\", sc.uiWebUrl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "ilYU73E4LJyL"
      },
      "outputs": [],
      "source": [
        "ngrok.kill()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_BqNhbHXLkXg",
        "outputId": "e7093720-a005-4a91-ef8e-32637abf05b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-11-26T01:46:10+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark UI: NgrokTunnel: \"https://bf18-34-86-2-41.ngrok-free.app\" -> \"http://localhost:4040\"\n"
          ]
        }
      ],
      "source": [
        "public_url = ngrok.connect(4040)\n",
        "print(\"Spark UI:\", public_url)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms"
      ],
      "metadata": {
        "id": "sgq627W68BTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
        "from pyspark.ml.clustering import BisectingKMeans, KMeans, GaussianMixture\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.classification import LogisticRegression, LinearSVC, RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator\n",
        "from pyspark.sql.functions import col, when\n",
        "from pyspark.sql.types import DoubleType\n",
        "import time\n",
        "\n",
        "# Sample the data\n",
        "sample_size = 10000\n",
        "df_sample = df.sample(withReplacement=False, fraction=sample_size/df.count(), seed=42)\n",
        "# Prepare features\n",
        "feature_cols = ['Recency', 'Frequency', 'Monetary']\n",
        "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
        "df_vector = assembler.transform(df_sample)\n",
        "# Scale features\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\")\n",
        "scaler_model = scaler.fit(df_vector)\n",
        "df_scaled = scaler_model.transform(df_vector)"
      ],
      "metadata": {
        "id": "HD4M5LUS2Hqi"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Hierarchical Clustering (Using BisectingKMeans as alternative since MLlib doesn't have hierarchical)\n",
        "start_time = time.time()\n",
        "bkm = BisectingKMeans(k=4, featuresCol=\"scaled_features\")\n",
        "model_bkm = bkm.fit(df_scaled)\n",
        "df_bkm = model_bkm.transform(df_scaled)\n",
        "evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"scaled_features\")\n",
        "silhouette_bkm = evaluator.evaluate(df_bkm)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Bisecting KMeans Silhouette Score: {silhouette_bkm:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLN768l0-ljY",
        "outputId": "9dd702b1-98e2-47ef-b53e-321e0fad7030"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bisecting KMeans Silhouette Score: 0.8470\n",
            "Execution Time: 112.6334 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. K-Means\n",
        "start_time = time.time()\n",
        "kmeans = KMeans(k=4, featuresCol=\"scaled_features\")\n",
        "model_kmeans = kmeans.fit(df_scaled)\n",
        "df_kmeans = model_kmeans.transform(df_scaled)\n",
        "silhouette_kmeans = evaluator.evaluate(df_kmeans)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"KMeans Silhouette Score: {silhouette_kmeans:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbARmNP6-mpL",
        "outputId": "49685518-9f45-4d78-dd02-5e55fe2ecea5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KMeans Silhouette Score: 0.8614\n",
            "Execution Time: 43.5863 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Linear Regression\n",
        "start_time = time.time()\n",
        "assembler_lr = VectorAssembler(inputCols=['Recency', 'Frequency'], outputCol=\"features\")\n",
        "df_lr = assembler_lr.transform(df_sample)\n",
        "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Monetary\")\n",
        "lr_model = lr.fit(df_lr)\n",
        "predictions_lr = lr_model.transform(df_lr)\n",
        "evaluator_lr = RegressionEvaluator(labelCol=\"Monetary\", predictionCol=\"prediction\")\n",
        "r2 = evaluator_lr.evaluate(predictions_lr, {evaluator_lr.metricName: \"r2\"})\n",
        "mse = evaluator_lr.evaluate(predictions_lr, {evaluator_lr.metricName: \"mse\"})\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Linear Regression R2: {r2:.4f}\")\n",
        "print(f\"Linear Regression MSE: {mse:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2pyF_Ci-qYI",
        "outputId": "db2013dc-e560-46a1-88c3-bc4f8ee34604"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Regression R2: 0.3686\n",
            "Linear Regression MSE: 620738043.0074\n",
            "Execution Time: 10.3524 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Logistic Regression\n",
        "start_time = time.time()\n",
        "df_sample = df_sample.withColumn(\"RFM_Score_numeric\", col(\"RFM_Score\").cast(DoubleType()))\n",
        "feature_cols_log = ['Recency', 'Frequency', 'Monetary', 'Total_Price', 'Quantity']\n",
        "assembler_log = VectorAssembler(inputCols=feature_cols_log, outputCol=\"features\")\n",
        "df_log = assembler_log.transform(df_sample)\n",
        "lr_classifier = LogisticRegression(featuresCol=\"features\", labelCol=\"RFM_Score_numeric\")\n",
        "lr_model = lr_classifier.fit(df_log)\n",
        "predictions_lr = lr_model.transform(df_log)\n",
        "# Evaluation metrics\n",
        "evaluator_lr = MulticlassClassificationEvaluator(labelCol=\"RFM_Score_numeric\", predictionCol=\"prediction\")\n",
        "accuracy_lr = evaluator_lr.evaluate(predictions_lr)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Logistic Regression Accuracy: {accuracy_lr:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9X6yqyh-wWt",
        "outputId": "d66faeb7-bb09-4e5c-f215-e5f0e70a0d53"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Accuracy: 0.7578\n",
            "Execution Time: 210.2223 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZjhbROi7mXa",
        "outputId": "35ae42f5-addc-4f01-849f-7b697642c15b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.8541\n",
            "Execution Time: 65.3989 seconds\n"
          ]
        }
      ],
      "source": [
        "# 5. Random Forest Model\n",
        "start_time = time.time()\n",
        "distinct_rfm_scores = df_log.select(\"RFM_Score_numeric\").distinct().collect()\n",
        "rfm_mapping = {row[\"RFM_Score_numeric\"]: idx for idx, row in enumerate(distinct_rfm_scores)}\n",
        "rfm_mapping_broadcast = sc.broadcast(rfm_mapping)\n",
        "mapping_expr = when(col(\"RFM_Score_numeric\").isNull(), None)\n",
        "for key, value in rfm_mapping_broadcast.value.items():\n",
        "    mapping_expr = mapping_expr.when(col(\"RFM_Score_numeric\") == key, value)\n",
        "df_log = df_log.withColumn(\"label\", mapping_expr)\n",
        "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=10)\n",
        "rf_model = rf.fit(df_log)\n",
        "predictions_rf = rf_model.transform(df_log)\n",
        "# Evaluation metrics\n",
        "evaluator_rf = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy_rf = evaluator_rf.evaluate(predictions_rf)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "uZRzXxqR9HCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc35798b-52d3-4622-8864-7752eaf08df5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Mixture Model Silhouette Score: 0.1937\n",
            "Execution Time: 106.1357 seconds\n"
          ]
        }
      ],
      "source": [
        "# 6. Gaussian Mixture Model (GMM)\n",
        "start_time = time.time()\n",
        "gmm = GaussianMixture(k=4, featuresCol=\"scaled_features\")\n",
        "model_gmm = gmm.fit(df_scaled)\n",
        "df_gmm = model_gmm.transform(df_scaled)\n",
        "silhouette_gmm = evaluator.evaluate(df_gmm)\n",
        "end_time = time.time()\n",
        "execution_time = end_time - start_time\n",
        "print(f\"Gaussian Mixture Model Silhouette Score: {silhouette_gmm:.4f}\")\n",
        "print(f\"Execution Time: {execution_time:.4f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **BONUS**"
      ],
      "metadata": {
        "id": "KRznq8_OSeu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://dicproject-yvcnsqljtsyh7tqccnpml9.streamlit.app/"
      ],
      "metadata": {
        "id": "kJdMBVjTSzRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above link is the streamlit app for the product developed for the customer segmentation using Kmeans Algorithm.\n",
        "\n"
      ],
      "metadata": {
        "id": "8yGGfC_GS5ZL"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}